# RAG with Pinecone & LangChain (JavaScript + Gemini) ğŸš€

A minimal, practical guide to Retrieval-Augmented Generation (RAG) using Pinecone for vector search and LangChain (JS) to orchestrate embeddings â†’ retrieval â†’ Gemini-powered generation. Short, focused, and ready to slot into your repo.

---

## âœ¨Features

- **Fast semantic search** with Pinecone
- **LangChain JS** to build retrieval + generation chains
- **Google Gemini** for embeddings and text generation
- **Simple provenance:** returns sources & chunk references

---

## ğŸ§°Tech Stack

- **Node.js (JavaScript)**
- **LangChain (JS)**
- **Google Gemini** (embeddings + text)
- **Pinecone** (vector DB)
- _Optional:_ S3 / local storage for raw docs

---

## Small Example (Conceptual)

**CONTEXT:**

- Chunk 1: â€œEmployees may work remotely up to 3 days/week (HR-Policy-2025.pdf, p.4).â€
- Chunk 2: â€œManager approval required for remote arrangements (HR-Policy-2025.pdf, p.5).â€

**QUESTION:**
> What are the 2025 remote-work rules?

**AUGMENTED PROMPT** (sent to Gemini):  
â€œUsing only the context above, answer: What are the 2025 remote-work rules?â€

**EXPECTED OUTPUT:**  
â€œEmployees may work remotely up to three days per week and must obtain manager approval. [source: HR-Policy-2025.pdf p.4â€“5]â€

---

## âš¡Quick Usage (3 Steps)

1. **Index documents:** chunk â†’ embed (Gemini) â†’ upsert into Pinecone.
2. **Query:** embed user query â†’ nearest-neighbor search in Pinecone â†’ build prompt with top-K chunks.
3. **Generate:** call Gemini with the augmented prompt and return answer + sources.

---

## ğŸ”‘ Important Environment Variables

- `GEMINI_API_KEY`
- `GEMINI_EMBEDDING_MODEL`
- `GEMINI_TEXT_MODEL`
- `PINECONE_API_KEY`
- `PINECONE_INDEX_NAME`

---

## Tips

- Keep temperature low (`0â€“0.3`) for factual answers.
- Use explicit instruction: â€œAnswer only using the context.â€
- Tune chunk size (200â€“800 tokens) and overlap for better retrieval.

---



---

Happy building â€” give the model a textbook and itâ€™ll stop guessing.
